{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must download the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "dataURL = \"http://www.statmt.org/europarl/v7/pt-en.tgz\"\n",
    "fileName = \"pt-en.tgz\"\n",
    "tarFile = Path(fileName)\n",
    "if tarFile.is_file():\n",
    "    print(\"Already downloaded\")\n",
    "else:\n",
    "    r = requests.get(dataURL)\n",
    "    with open(fileName, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must extract the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "enFile = Path(\"europarl-v7.pt-en.en\")\n",
    "ptFile = Path(\"europarl-v7.pt-en.pt\")\n",
    "if enFile.is_file() and ptFile.is_file():\n",
    "    print(\"Already extracted\")\n",
    "else:\n",
    "    with tarfile.open(fileName) as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create our vocabularies in both languages. Lets go with something small to begin with, how about 20,000 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download() # d all (I really just want the tokenizer's .pickle for both languages, but I don't know where it is so just download everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(sentance, language):\n",
    "    tokens = word_tokenize(sentance.decode('utf-8'), language=language)\n",
    "    return [word.lower() for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "startVocab = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "def createVocabulary(corpusFile, vocabFile, numWords, language):\n",
    "    vocab = {}\n",
    "    with open(corpusFile) as corpus:\n",
    "      for line in corpus:\n",
    "        words = tokenize(sentance, language)\n",
    "        for word in words:\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      orderedWords = startVocab + sorted(vocab, key=vocab.get, reverse=True)\n",
    "      if len(orderedWords) > numWords:\n",
    "        orderedWords = orderedWords[:numWords]\n",
    "      with open(vocabFile, 'wu') as file:\n",
    "        for word in orderedWords:\n",
    "          file.write(word.encode('utf-8') + \"\\n\")\n",
    "        \n",
    "enVocab = Path(\"vocab3.en\")\n",
    "ptVocab = Path(\"vocab3.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    print(\"Starting English vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.en\", \"vocab3.en\", 40000, \"english\")\n",
    "    print(\"Finished English vocab\")\n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.pt\", \"vocab3.pt\", 40000, \"portuguese\")\n",
    "    print(\"Finished Portuguese vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "def mapCorpus(corpusFile, vocabFile, mappedFile, language):\n",
    "    words = []\n",
    "    with open(vocabFile) as v:\n",
    "      words.extend(v.readlines())\n",
    "    words = [line.strip() for line in words]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(words)])\n",
    "    with open(corpusFile) as corpus:\n",
    "        with open(mappedFile, \"w\") as output:\n",
    "            for line in corpus:\n",
    "                tokens = word_tokenize(line.decode('utf-8'), language=language)\n",
    "                words = [word.lower() for word in tokens if word.isalpha()]\n",
    "                ids = [str(vocab.get(word, UNK_ID)) for word in words]\n",
    "                output.write(\" \".join(ids) + \"\\n\")\n",
    "\n",
    "enVocab = Path(\"mapped3.en\")\n",
    "ptVocab = Path(\"mapped3.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    print(\"Starting English map\")\n",
    "    mapCorpus(\"europarl-v7.pt-en.en\", \"vocab3.en\", \"mapped3.en\", \"english\")\n",
    "    \n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese map\")\n",
    "    mapCorpus(\"europarl-v7.pt-en.pt\", \"vocab3.pt\", \"mapped3.pt\", \"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "def splitIntoDevAndTraining(mappedFile, numberOfDevExamples, devOut, trainOut):\n",
    "    with open(mappedFile) as mapped:\n",
    "        with open(devOut, \"w\") as dev:\n",
    "            with open(trainOut, \"w\") as train:\n",
    "                count = 0\n",
    "                for line in mapped.readlines():\n",
    "                    count += 1\n",
    "                    if count < numberOfDevExamples:\n",
    "                        dev.write(line)\n",
    "                    else:\n",
    "                        train.write(line)\n",
    "\n",
    "enDev = Path(\"dev3.en\")\n",
    "ptDev = Path(\"dev3.pt\")\n",
    "enTrain = Path(\"train3.en\")\n",
    "ptTrain = Path(\"train3.pt\")\n",
    "\n",
    "if enDev.is_file() and enTrain.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    print(\"Starting English split\")\n",
    "    splitIntoDevAndTraining(\"mapped3.en\", 2000, \"dev3.en\", \"train3.en\")\n",
    "    \n",
    "if ptDev.is_file() and ptTrain.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese split\")\n",
    "    splitIntoDevAndTraining(\"mapped3.pt\", 2000, \"dev3.pt\", \"train3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  dtype = tf.float32\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size,\n",
    "      FLAGS.pt_vocab_size,\n",
    "      _buckets,\n",
    "      FLAGS.size,\n",
    "      FLAGS.num_layers,\n",
    "      FLAGS.max_gradient_norm,\n",
    "      FLAGS.batch_size,\n",
    "      FLAGS.learning_rate,\n",
    "      FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "  ckpt = tf.train.get_checkpoint_state(\"checkpoint4\")\n",
    "  if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.global_variables_initializer())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"pt_vocab_size\", 40000, \"Portuguese vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import sys\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with open(source_path) as source_file:\n",
    "    with open(target_path) as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def train():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "    summary_writer = tf.train.FileWriter('checkpoint4', graph=sess.graph)\n",
    "\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    dev_data = read_data(\"dev3.en\", \"dev3.pt\", 0)\n",
    "    train_data = read_data(\"train3.en\", \"train3.pt\", FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_data[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_data, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "#         summary_writer.\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "\n",
    "        print(\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        stats = open(\"stats.txt\", \"w\")\n",
    "        stats.write(\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = \"checkpoint4/translate.ckpt\"\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(dev_data[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_data, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\n",
    "              \"inf\")\n",
    "          stats.write(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        stats.close()\n",
    "      sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 3 layers of 1024 units.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/seq2seq_model.py:188 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "Reading model parameters from checkpoint4/translate.ckpt-172200\n",
      "WARNING:tensorflow:From <ipython-input-11-29a7f81a8760>:34 in train.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "Reading development and training data (limit: 0).\n",
      "  reading data line 100000\n",
      "  reading data line 200000\n",
      "  reading data line 300000\n",
      "  reading data line 400000\n",
      "  reading data line 500000\n",
      "  reading data line 600000\n",
      "  reading data line 700000\n",
      "  reading data line 800000\n",
      "  reading data line 900000\n",
      "  reading data line 1000000\n",
      "  reading data line 1100000\n",
      "  reading data line 1200000\n",
      "  reading data line 1300000\n",
      "  reading data line 1400000\n",
      "  reading data line 1500000\n",
      "  reading data line 1600000\n",
      "  reading data line 1700000\n",
      "  reading data line 1800000\n",
      "  reading data line 1900000\n",
      "global step 172400 learning rate 0.1012 step-time 0.98 perplexity 3.17\n",
      "  eval: bucket 0 perplexity 1.53\n",
      "  eval: bucket 1 perplexity 3.32\n",
      "  eval: bucket 2 perplexity 5.50\n",
      "  eval: bucket 3 perplexity 6.16\n",
      "global step 172600 learning rate 0.1012 step-time 0.90 perplexity 3.03\n",
      "  eval: bucket 0 perplexity 1.54\n",
      "  eval: bucket 1 perplexity 2.88\n",
      "  eval: bucket 2 perplexity 4.57\n",
      "  eval: bucket 3 perplexity 5.66\n",
      "global step 172800 learning rate 0.1012 step-time 0.87 perplexity 3.05\n",
      "  eval: bucket 0 perplexity 1.48\n",
      "  eval: bucket 1 perplexity 3.23\n",
      "  eval: bucket 2 perplexity 4.76\n",
      "  eval: bucket 3 perplexity 6.23\n",
      "  eval: bucket 0 perplexity 1.40\n",
      "  eval: bucket 1 perplexity 3.63\n",
      "  eval: bucket 2 perplexity 4.82\n",
      "  eval: bucket 3 perplexity 6.61\n",
      "global step 176000 learning rate 0.0981 step-time 0.86 perplexity 2.89\n",
      "  eval: bucket 0 perplexity 1.42\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(sentances):\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    enWords = []\n",
    "    with open(\"vocab3.en\") as enVocabFile:\n",
    "        enWords.extend(enVocabFile.readlines())\n",
    "    enWords = [line.strip() for line in enWords]\n",
    "    enVocab = dict([(x, y) for (y, x) in enumerate(enWords)])\n",
    "    \n",
    "    ptWords = []\n",
    "    with open(\"vocab3.pt\") as ptVocabFile:\n",
    "        ptWords.extend(ptVocabFile.readlines())\n",
    "    ptWords = [line.strip() for line in ptWords]\n",
    "    ptVocab = dict(enumerate(ptWords))\n",
    "    print(\"Starting translation\")\n",
    "    for sentance in sentances:\n",
    "                # Get token-ids for the input sentence.\n",
    "                tokens = tokenize(sentance, \"english\")\n",
    "                tokenIds = [enVocab.get(t, UNK_ID) for t in tokens]\n",
    "                if len(tokenIds) > _buckets[len(_buckets) - 1][0]:\n",
    "                    continue\n",
    "                # Which bucket does it belong to?\n",
    "                bucket_id = len(_buckets) - 1\n",
    "                for i, bucket in enumerate(_buckets):\n",
    "                    if bucket[0] >= len(tokenIds):\n",
    "                        bucket_id = i\n",
    "                        break\n",
    "\n",
    "                # Get a 1-element batch to feed the sentence to the model.\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "                    {bucket_id: [(tokenIds, [])]}, bucket_id)\n",
    "                # Get output logits for the sentence.\n",
    "                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                                 target_weights, bucket_id, True)\n",
    "                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "                # If there is an EOS symbol in outputs, cut them at that point.\n",
    "                if EOS_ID in outputs:\n",
    "                    outputs = outputs[:outputs.index(EOS_ID)]\n",
    "                # Print out Portuguese sentence corresponding to outputs.\n",
    "                print(sentance)\n",
    "                print(tokens)\n",
    "                print(\" \".join([tf.compat.as_str(ptVocab[output]) for output in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition between the regions will certainly strengthen rather than weaken the European Union.\n",
      "[u'competition', u'between', u'the', u'regions', u'will', u'certainly', u'strengthen', u'rather', u'than', u'weaken', u'the', u'european', u'union']\n",
      "It was this which inspired us to propose the same thing with regard to state aid.\n",
      "[u'it', u'was', u'this', u'which', u'inspired', u'us', u'to', u'propose', u'the', u'same', u'thing', u'with', u'regard', u'to', u'state', u'aid']\n",
      "During the course of our work on this report it became clear that there are persistent problems in the spending areas under the control of our budget.\n",
      "[u'during', u'the', u'course', u'of', u'our', u'work', u'on', u'this', u'report', u'it', u'became', u'clear', u'that', u'there', u'are', u'persistent', u'problems', u'in', u'the', u'spending', u'areas', u'under', u'the', u'control', u'of', u'our', u'budget']\n",
      "If the establishment of such an area, in which the Union can also intervene in the basic rights of the citizens, is only decided by diplomats and bureaucrats, while the elected representatives of Europe are reduced to following developments like a rabbit watching a snake, then this area will not gain the acceptance of the citizens.\n",
      "[u'if', u'the', u'establishment', u'of', u'such', u'an', u'area', u'in', u'which', u'the', u'union', u'can', u'also', u'intervene', u'in', u'the', u'basic', u'rights', u'of', u'the', u'citizens', u'is', u'only', u'decided', u'by', u'diplomats', u'and', u'bureaucrats', u'while', u'the', u'elected', u'representatives', u'of', u'europe', u'are', u'reduced', u'to', u'following', u'developments', u'like', u'a', u'rabbit', u'watching', u'a', u'snake', u'then', u'this', u'area', u'will', u'not', u'gain', u'the', u'acceptance', u'of', u'the', u'citizens']\n",
      "All asylum-seekers must be entitled to a fair hearing and an appeal with suspensory effect.\n",
      "[u'all', u'must', u'be', u'entitled', u'to', u'a', u'fair', u'hearing', u'and', u'an', u'appeal', u'with', u'suspensory', u'effect']\n",
      "Nevertheless, to avoid the customary circus, which every six months has us indiscriminately attacking the Presidency of the Council, maybe Parliament should find the courage to undertake incisive political action so that the next Intergovernmental Conference will decide to increase codecision with immediate effect and not wait for another five years.\n",
      "[u'nevertheless', u'to', u'avoid', u'the', u'customary', u'circus', u'which', u'every', u'six', u'months', u'has', u'us', u'indiscriminately', u'attacking', u'the', u'presidency', u'of', u'the', u'council', u'maybe', u'parliament', u'should', u'find', u'the', u'courage', u'to', u'undertake', u'incisive', u'political', u'action', u'so', u'that', u'the', u'next', u'intergovernmental', u'conference', u'will', u'decide', u'to', u'increase', u'codecision', u'with', u'immediate', u'effect', u'and', u'not', u'wait', u'for', u'another', u'five', u'years']\n",
      "But I want to add a word of caution.\n",
      "[u'but', u'i', u'want', u'to', u'add', u'a', u'word', u'of', u'caution']\n",
      "All of this must nevertheless be done whilst respecting a broad framework - that of the overall acceptability of any solutions we may devise to achieve these three aims.\n",
      "[u'all', u'of', u'this', u'must', u'nevertheless', u'be', u'done', u'whilst', u'respecting', u'a', u'broad', u'framework', u'that', u'of', u'the', u'overall', u'acceptability', u'of', u'any', u'solutions', u'we', u'may', u'devise', u'to', u'achieve', u'these', u'three', u'aims']\n",
      "We are looking for solutions.\n",
      "[u'we', u'are', u'looking', u'for', u'solutions']\n",
      "Madam President, on a point of order.\n",
      "[u'madam', u'president', u'on', u'a', u'point', u'of', u'order']\n",
      "I declare the session of the European Parliament adjourned.\n",
      "[u'i', u'declare', u'the', u'session', u'of', u'the', u'european', u'parliament', u'adjourned']\n"
     ]
    }
   ],
   "source": [
    "decode([\"Competition between the regions will certainly strengthen rather than weaken the European Union.\",\n",
    "        \"It was this which inspired us to propose the same thing with regard to state aid.\",\n",
    "       \"During the course of our work on this report it became clear that there are persistent problems in the spending areas under the control of our budget.\",\n",
    "       \"If the establishment of such an area, in which the Union can also intervene in the basic rights of the citizens, is only decided by diplomats and bureaucrats, while the elected representatives of Europe are reduced to following developments like a rabbit watching a snake, then this area will not gain the acceptance of the citizens.\",\n",
    "        \"All asylum-seekers must be entitled to a fair hearing and an appeal with suspensory effect.\",\n",
    "        \"Nevertheless, to avoid the customary circus, which every six months has us indiscriminately attacking the Presidency of the Council, maybe Parliament should find the courage to undertake incisive political action so that the next Intergovernmental Conference will decide to increase codecision with immediate effect and not wait for another five years.\",\n",
    "        \"But I want to add a word of caution.\",\n",
    "        \"All of this must nevertheless be done whilst respecting a broad framework - that of the overall acceptability of any solutions we may devise to achieve these three aims.\",\n",
    "        \"We are looking for solutions.\",\n",
    "        \"Madam President, on a point of order.\",\n",
    "        \"I declare the session of the European Parliament adjourned.\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
