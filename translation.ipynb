{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must download the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "dataURL = \"http://www.statmt.org/europarl/v7/pt-en.tgz\"\n",
    "fileName = \"pt-en.tgz\"\n",
    "tarFile = Path(fileName)\n",
    "if tarFile.is_file():\n",
    "    print(\"Already downloaded\")\n",
    "else:\n",
    "    r = requests.get(dataURL)\n",
    "    with open(fileName, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must extract the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "enFile = Path(\"europarl-v7.pt-en.en\")\n",
    "ptFile = Path(\"europarl-v7.pt-en.pt\")\n",
    "if enFile.is_file() and ptFile.is_file():\n",
    "    print(\"Already extracted\")\n",
    "else:\n",
    "    with tarfile.open(fileName) as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create our vocabularies in both languages. Lets go with something small to begin with, how about 20,000 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "startVocab = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "def createVocabulary(corpusFile, vocabFile, numWords, language):\n",
    "    vocab = {}\n",
    "    with open(corpusFile) as corpus:\n",
    "      for line in corpus:\n",
    "        tokens = word_tokenize(line, language=language)\n",
    "        for word in tokens:\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      orderedWords = startVocab + sorted(vocab, key=vocab.get, reverse=True)\n",
    "      if len(orderedWords) > numWords:\n",
    "        orderedWords = orderedWords[:numWords]\n",
    "      with open(vocabFile, 'w') as file:\n",
    "        for word in orderedWords:\n",
    "          file.write(word + \"\\n\")\n",
    "\n",
    "enVocab = Path(\"vocab.en\")\n",
    "ptVocab = Path(\"vocab.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    print(\"Starting English vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.en\", \"vocab.en\", 20000, \"english\")\n",
    "    print(\"Finished English vocab\")\n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.pt\", \"vocab.pt\", 20000, \"portuguese\")\n",
    "    print(\"Finished Portuguese vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "def mapCorpus(corpusFile, vocabFile, mappedFile, language):\n",
    "    words = []\n",
    "    with open(vocabFile) as v:\n",
    "      words.extend(v.readlines())\n",
    "    words = [line.strip() for line in words]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(words)])\n",
    "    with open(corpusFile) as corpus:\n",
    "        with open(mappedFile, \"w\") as output:\n",
    "            for line in corpus:\n",
    "                tokens = word_tokenize(line, language=language)\n",
    "                ids = [str(vocab.get(token, UNK_ID)) for token in tokens]\n",
    "                output.write(\" \".join(ids) + \"\\n\")\n",
    "\n",
    "enVocab = Path(\"mapped.en\")\n",
    "ptVocab = Path(\"mapped.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    mapCorpus(\"europarl-v7.pt-en.en\", \"vocab.en\", \"mapped.en\", \"english\")\n",
    "    \n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese map\")\n",
    "    mapCorpus(\"europarl-v7.pt-en.pt\", \"vocab.pt\", \"mapped.pt\", \"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  dtype = tf.float32\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size,\n",
    "      FLAGS.pt_vocab_size,\n",
    "      _buckets,\n",
    "      FLAGS.size,\n",
    "      FLAGS.num_layers,\n",
    "      FLAGS.max_gradient_norm,\n",
    "      FLAGS.batch_size,\n",
    "      FLAGS.learning_rate,\n",
    "      FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "  ckpt = tf.train.get_checkpoint_state(\"checkpoints\")\n",
    "  if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 512, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 20000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"pt_vocab_size\", 20000, \"Portuguese vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import sys\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with open(source_path) as source_file:\n",
    "    with open(target_path) as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def train():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    data = read_data(\"mapped.en\", \"mapped.pt\", FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(data[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = \"checkpoint/translate.ckpt\"\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(data[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              data, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\n",
    "              \"inf\")\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoint: File exists\n",
      "Creating 3 layers of 512 units.\n",
      "Created model with fresh parameters.\n",
      "Reading development and training data (limit: 0).\n",
      "  reading data line 100000\n",
      "  reading data line 200000\n",
      "  reading data line 300000\n",
      "  reading data line 400000\n",
      "  reading data line 500000\n",
      "  reading data line 600000\n",
      "  reading data line 700000\n",
      "  reading data line 800000\n",
      "  reading data line 900000\n",
      "  reading data line 1000000\n",
      "  reading data line 1100000\n",
      "  reading data line 1200000\n",
      "  reading data line 1300000\n",
      "  reading data line 1400000\n",
      "  reading data line 1500000\n",
      "  reading data line 1600000\n",
      "  reading data line 1700000\n",
      "  reading data line 1800000\n",
      "  reading data line 1900000\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint\n",
    "train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:final]",
   "language": "python",
   "name": "conda-env-final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
