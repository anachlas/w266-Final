{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must download the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "dataURL = \"http://www.statmt.org/europarl/v7/pt-en.tgz\"\n",
    "fileName = \"pt-en.tgz\"\n",
    "tarFile = Path(fileName)\n",
    "if tarFile.is_file():\n",
    "    print(\"Already downloaded\")\n",
    "else:\n",
    "    r = requests.get(dataURL)\n",
    "    with open(fileName, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(sentance, language):\n",
    "    tokens = word_tokenize(sentance.decode('utf-8'), language=language)\n",
    "    return [word.lower() for word in tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-eda756005a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mgetLineLengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"europarl-v7.pt-en.en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linelen.en\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mgetLineLengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"europarl-v7.pt-en.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"portuguese\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"linelen.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-eda756005a13>\u001b[0m in \u001b[0;36mgetLineLengths\u001b[0;34m(corpusFile, language, outfile)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpusFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     return [token for sent in sent_tokenize(text, language)\n\u001b[0;32m--> 110\u001b[0;31m             for token in _treebank_word_tokenize(sent)]\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# We are not using CONTRACTIONS4 since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getLineLengths(corpusFile, language, outfile):\n",
    "    lengths = {}\n",
    "    with open(corpusFile) as corpus:\n",
    "      for line in corpus:\n",
    "        tokens = word_tokenize(line, language=language)\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        length = len(words)\n",
    "        if length in lengths:\n",
    "          lengths[length] += 1\n",
    "        else:\n",
    "          lengths[length] = 1\n",
    "    with open(outfile, \"w\") as outfile:\n",
    "      for length,count in lengths.items():\n",
    "        outfile.write(str(length) + \",\" + str(count) + \"\\n\")\n",
    "      \n",
    "getLineLengths(\"europarl-v7.pt-en.en\", \"english\", \"linelen.en\")\n",
    "getLineLengths(\"europarl-v7.pt-en.pt\", \"portuguese\", \"linelen.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must extract the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already extracted\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "enFile = Path(\"europarl-v7.pt-en.en\")\n",
    "ptFile = Path(\"europarl-v7.pt-en.pt\")\n",
    "if enFile.is_file() and ptFile.is_file():\n",
    "    print(\"Already extracted\")\n",
    "else:\n",
    "    with tarfile.open(fileName) as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create our vocabularies in both languages. Lets go with something small to begin with, how about 20,000 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "startVocab = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "def createVocabulary(corpusFile, vocabFile, numWords, language):\n",
    "    vocab = {}\n",
    "    with open(corpusFile) as corpus:\n",
    "      for line in corpus:\n",
    "        tokens = word_tokenize(line, language=language)\n",
    "        for word in tokens:\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      orderedWords = startVocab + sorted(vocab, key=vocab.get, reverse=True)\n",
    "      if len(orderedWords) > numWords:\n",
    "        orderedWords = orderedWords[:numWords]\n",
    "      with open(vocabFile, 'w') as file:\n",
    "        for word in orderedWords:\n",
    "          file.write(word + \"\\n\")\n",
    "\n",
    "\n",
    "def createVocabularyCount(corpusFile, countFile, language):\n",
    "    vocab = {}\n",
    "    with open(corpusFile) as corpus:\n",
    "      for line in corpus:\n",
    "        words = word_tokenize(line, language=language)\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        for word in words:\n",
    "          if word in vocab:\n",
    "            vocab[word] += 1\n",
    "          else:\n",
    "            vocab[word] = 1\n",
    "      with open(countFile, 'w') as file:\n",
    "        for word, count in vocab.items():\n",
    "          file.write(word + \"\\t\" + str(count) + \"\\n\")\n",
    "\n",
    "# createVocabularyCount(\"europarl-v7.pt-en.en\", \"counts2.en\", \"english\")\n",
    "# createVocabularyCount(\"europarl-v7.pt-en.pt\", \"counts2.pt\", \"portuguese\")\n",
    "\n",
    "enVocab = Path(\"vocab.en\")\n",
    "ptVocab = Path(\"vocab.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    print(\"Starting English vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.en\", \"vocab.en\", 20000, \"english\")\n",
    "    print(\"Finished English vocab\")\n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese vocab\")\n",
    "    createVocabulary(\"europarl-v7.pt-en.pt\", \"vocab.pt\", 20000, \"portuguese\")\n",
    "    print(\"Finished Portuguese vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already did English\n",
      "Already did Portuguese\n"
     ]
    }
   ],
   "source": [
    "def mapCorpus(corpusFile, vocabFile, mappedFile, language):\n",
    "    words = []\n",
    "    with open(vocabFile) as v:\n",
    "      words.extend(v.readlines())\n",
    "    words = [line.strip() for line in words]\n",
    "    vocab = dict([(x, y) for (y, x) in enumerate(words)])\n",
    "    with open(corpusFile) as corpus:\n",
    "        with open(mappedFile, \"w\") as output:\n",
    "            for line in corpus:\n",
    "                tokens = word_tokenize(line, language=language)\n",
    "                ids = [str(vocab.get(token, UNK_ID)) for token in tokens]\n",
    "                output.write(\" \".join(ids) + \"\\n\")\n",
    "\n",
    "enVocab = Path(\"mapped.en\")\n",
    "ptVocab = Path(\"mapped.pt\")\n",
    "\n",
    "if enVocab.is_file():\n",
    "    print(\"Already did English\")\n",
    "else:\n",
    "    mapCorpus(\"europarl-v7.pt-en.en\", \"vocab.en\", \"mapped.en\", \"english\")\n",
    "    \n",
    "if ptVocab.is_file():\n",
    "    print(\"Already did Portuguese\")\n",
    "else:\n",
    "    print(\"Starting Portuguese map\")\n",
    "    mapCorpus(\"europarl-v7.pt-en.pt\", \"vocab.pt\", \"mapped.pt\", \"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  dtype = tf.float32\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size,\n",
    "      FLAGS.pt_vocab_size,\n",
    "      _buckets,\n",
    "      FLAGS.size,\n",
    "      FLAGS.num_layers,\n",
    "      FLAGS.max_gradient_norm,\n",
    "      FLAGS.batch_size,\n",
    "      FLAGS.learning_rate,\n",
    "      FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "  ckpt = tf.train.get_checkpoint_state(\"checkpoint\")\n",
    "  if ckpt:\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 512, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 20000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"pt_vocab_size\", 20000, \"Portuguese vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import sys\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def read_data(source_path, target_path, max_size=None):\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with open(source_path) as source_file:\n",
    "    with open(target_path) as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n",
    "\n",
    "def train():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    data = read_data(\"mapped.en\", \"mapped.pt\", FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(data[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = \"checkpoint/translate.ckpt\"\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(data[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              data, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\n",
    "              \"inf\")\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoint: File exists\n",
      "Creating 3 layers of 512 units.\n",
      "Created model with fresh parameters.\n",
      "Reading development and training data (limit: 0).\n",
      "  reading data line 100000\n",
      "  reading data line 200000\n",
      "  reading data line 300000\n",
      "  reading data line 400000\n",
      "  reading data line 500000\n",
      "  reading data line 600000\n",
      "  reading data line 700000\n",
      "  reading data line 800000\n",
      "  reading data line 900000\n",
      "  reading data line 1000000\n",
      "  reading data line 1100000\n",
      "  reading data line 1200000\n",
      "  reading data line 1300000\n",
      "  reading data line 1400000\n",
      "  reading data line 1500000\n",
      "  reading data line 1600000\n",
      "  reading data line 1700000\n",
      "  reading data line 1800000\n",
      "  reading data line 1900000\n",
      "global step 200 learning rate 0.5000 step-time 5.10 perplexity 1916.38\n",
      "  eval: bucket 0 perplexity 98.63\n",
      "  eval: bucket 1 perplexity 243.48\n",
      "  eval: bucket 2 perplexity 293.95\n",
      "  eval: bucket 3 perplexity 312.79\n",
      "global step 400 learning rate 0.5000 step-time 4.99 perplexity 329.47\n",
      "  eval: bucket 0 perplexity 84.29\n",
      "  eval: bucket 1 perplexity 123.30\n",
      "  eval: bucket 2 perplexity 201.14\n",
      "  eval: bucket 3 perplexity 258.64\n",
      "global step 600 learning rate 0.5000 step-time 5.07 perplexity 242.77\n",
      "  eval: bucket 0 perplexity 498.85\n",
      "  eval: bucket 1 perplexity 207.98\n",
      "  eval: bucket 2 perplexity 251.93\n",
      "  eval: bucket 3 perplexity 222.36\n",
      "global step 800 learning rate 0.5000 step-time 5.18 perplexity 248.73\n",
      "  eval: bucket 0 perplexity 29.90\n",
      "  eval: bucket 1 perplexity 68.37\n",
      "  eval: bucket 2 perplexity 186.78\n",
      "  eval: bucket 3 perplexity 232.62\n",
      "global step 1000 learning rate 0.5000 step-time 5.15 perplexity 170.87\n",
      "  eval: bucket 0 perplexity 24.32\n",
      "  eval: bucket 1 perplexity 75.65\n",
      "  eval: bucket 2 perplexity 157.62\n",
      "  eval: bucket 3 perplexity 240.03\n",
      "global step 1200 learning rate 0.5000 step-time 5.05 perplexity 154.76\n",
      "  eval: bucket 0 perplexity 41.17\n",
      "  eval: bucket 1 perplexity 235.88\n",
      "  eval: bucket 2 perplexity 254.64\n",
      "  eval: bucket 3 perplexity 219.76\n",
      "global step 1400 learning rate 0.5000 step-time 5.13 perplexity 177.61\n",
      "  eval: bucket 0 perplexity 1139.75\n",
      "  eval: bucket 1 perplexity 1449.58\n",
      "  eval: bucket 2 perplexity 5854.70\n",
      "  eval: bucket 3 perplexity 146419.33\n",
      "global step 1600 learning rate 0.5000 step-time 5.11 perplexity 151.04\n",
      "  eval: bucket 0 perplexity 11.44\n",
      "  eval: bucket 1 perplexity 53.84\n",
      "  eval: bucket 2 perplexity 112.88\n",
      "  eval: bucket 3 perplexity 158.40\n",
      "global step 1800 learning rate 0.5000 step-time 5.14 perplexity 129.11\n",
      "  eval: bucket 0 perplexity 16.49\n",
      "  eval: bucket 1 perplexity 56.59\n",
      "  eval: bucket 2 perplexity 117.46\n",
      "  eval: bucket 3 perplexity 158.70\n",
      "global step 2000 learning rate 0.5000 step-time 4.75 perplexity 137.11\n",
      "  eval: bucket 0 perplexity 24.71\n",
      "  eval: bucket 1 perplexity 46.43\n",
      "  eval: bucket 2 perplexity 92.93\n",
      "  eval: bucket 3 perplexity 155.74\n",
      "global step 2200 learning rate 0.5000 step-time 5.12 perplexity 129.29\n",
      "  eval: bucket 0 perplexity 16.39\n",
      "  eval: bucket 1 perplexity 34.97\n",
      "  eval: bucket 2 perplexity 78.61\n",
      "  eval: bucket 3 perplexity 140.35\n",
      "global step 2400 learning rate 0.5000 step-time 5.18 perplexity 101.56\n",
      "  eval: bucket 0 perplexity 9.77\n",
      "  eval: bucket 1 perplexity 36.42\n",
      "  eval: bucket 2 perplexity 76.77\n",
      "  eval: bucket 3 perplexity 113.78\n",
      "global step 2600 learning rate 0.5000 step-time 5.31 perplexity 95.67\n",
      "  eval: bucket 0 perplexity 48.35\n",
      "  eval: bucket 1 perplexity 41.84\n",
      "  eval: bucket 2 perplexity 91.29\n",
      "  eval: bucket 3 perplexity 132.47\n",
      "global step 2800 learning rate 0.5000 step-time 5.17 perplexity 89.48\n",
      "  eval: bucket 0 perplexity 6.62\n",
      "  eval: bucket 1 perplexity 29.83\n",
      "  eval: bucket 2 perplexity 66.94\n",
      "  eval: bucket 3 perplexity 102.82\n",
      "global step 3000 learning rate 0.5000 step-time 5.20 perplexity 73.02\n",
      "  eval: bucket 0 perplexity 6.75\n",
      "  eval: bucket 1 perplexity 21.46\n",
      "  eval: bucket 2 perplexity 58.56\n",
      "  eval: bucket 3 perplexity 89.62\n",
      "global step 3200 learning rate 0.5000 step-time 5.01 perplexity 63.51\n",
      "  eval: bucket 0 perplexity 5.66\n",
      "  eval: bucket 1 perplexity 19.81\n",
      "  eval: bucket 2 perplexity 52.04\n",
      "  eval: bucket 3 perplexity 80.47\n",
      "global step 3400 learning rate 0.5000 step-time 5.04 perplexity 60.75\n",
      "  eval: bucket 0 perplexity 5.30\n",
      "  eval: bucket 1 perplexity 17.19\n",
      "  eval: bucket 2 perplexity 51.83\n",
      "  eval: bucket 3 perplexity 75.10\n",
      "global step 3600 learning rate 0.5000 step-time 5.10 perplexity 54.79\n",
      "  eval: bucket 0 perplexity 20.41\n",
      "  eval: bucket 1 perplexity 19.17\n",
      "  eval: bucket 2 perplexity 43.69\n",
      "  eval: bucket 3 perplexity 69.07\n",
      "global step 3800 learning rate 0.5000 step-time 4.94 perplexity 52.74\n",
      "  eval: bucket 0 perplexity 3.12\n",
      "  eval: bucket 1 perplexity 14.44\n",
      "  eval: bucket 2 perplexity 49.40\n",
      "  eval: bucket 3 perplexity 68.41\n",
      "global step 4000 learning rate 0.5000 step-time 5.40 perplexity 52.59\n",
      "  eval: bucket 0 perplexity 4.05\n",
      "  eval: bucket 1 perplexity 15.92\n",
      "  eval: bucket 2 perplexity 50.04\n",
      "  eval: bucket 3 perplexity 62.65\n",
      "global step 4200 learning rate 0.5000 step-time 4.82 perplexity 47.63\n",
      "  eval: bucket 0 perplexity 4.50\n",
      "  eval: bucket 1 perplexity 17.14\n",
      "  eval: bucket 2 perplexity 34.85\n",
      "  eval: bucket 3 perplexity 61.00\n",
      "global step 4400 learning rate 0.5000 step-time 5.25 perplexity 44.53\n",
      "  eval: bucket 0 perplexity 3.15\n",
      "  eval: bucket 1 perplexity 14.52\n",
      "  eval: bucket 2 perplexity 37.62\n",
      "  eval: bucket 3 perplexity 63.78\n",
      "global step 4600 learning rate 0.5000 step-time 5.06 perplexity 40.88\n",
      "  eval: bucket 0 perplexity 3.17\n",
      "  eval: bucket 1 perplexity 11.06\n",
      "  eval: bucket 2 perplexity 41.64\n",
      "  eval: bucket 3 perplexity 56.19\n",
      "global step 4800 learning rate 0.5000 step-time 4.71 perplexity 36.23\n",
      "  eval: bucket 0 perplexity 4.12\n",
      "  eval: bucket 1 perplexity 10.58\n",
      "  eval: bucket 2 perplexity 34.63\n",
      "  eval: bucket 3 perplexity 50.89\n",
      "global step 5000 learning rate 0.5000 step-time 5.39 perplexity 37.43\n",
      "  eval: bucket 0 perplexity 3.01\n",
      "  eval: bucket 1 perplexity 13.53\n",
      "  eval: bucket 2 perplexity 37.86\n",
      "  eval: bucket 3 perplexity 47.83\n",
      "global step 5200 learning rate 0.5000 step-time 5.42 perplexity 36.35\n",
      "  eval: bucket 0 perplexity 3.08\n",
      "  eval: bucket 1 perplexity 12.25\n",
      "  eval: bucket 2 perplexity 26.41\n",
      "  eval: bucket 3 perplexity 42.76\n",
      "global step 5400 learning rate 0.5000 step-time 5.38 perplexity 33.30\n",
      "  eval: bucket 0 perplexity 3.63\n",
      "  eval: bucket 1 perplexity 10.01\n",
      "  eval: bucket 2 perplexity 28.92\n",
      "  eval: bucket 3 perplexity 43.03\n",
      "global step 5600 learning rate 0.5000 step-time 5.36 perplexity 32.20\n",
      "  eval: bucket 0 perplexity 3.64\n",
      "  eval: bucket 1 perplexity 9.33\n",
      "  eval: bucket 2 perplexity 25.99\n",
      "  eval: bucket 3 perplexity 42.88\n",
      "global step 5800 learning rate 0.5000 step-time 5.42 perplexity 29.34\n",
      "  eval: bucket 0 perplexity 3.62\n",
      "  eval: bucket 1 perplexity 11.28\n",
      "  eval: bucket 2 perplexity 25.33\n",
      "  eval: bucket 3 perplexity 44.23\n",
      "global step 6000 learning rate 0.5000 step-time 5.59 perplexity 54.48\n",
      "  eval: bucket 0 perplexity 2.93\n",
      "  eval: bucket 1 perplexity 13.05\n",
      "  eval: bucket 2 perplexity 36.15\n",
      "  eval: bucket 3 perplexity 44.78\n",
      "global step 6200 learning rate 0.4950 step-time 5.59 perplexity 29.99\n",
      "  eval: bucket 0 perplexity 2.47\n",
      "  eval: bucket 1 perplexity 7.58\n",
      "  eval: bucket 2 perplexity 21.93\n",
      "  eval: bucket 3 perplexity 35.00\n",
      "global step 6400 learning rate 0.4950 step-time 5.51 perplexity 27.31\n",
      "  eval: bucket 0 perplexity 2.85\n",
      "  eval: bucket 1 perplexity 8.39\n",
      "  eval: bucket 2 perplexity 21.16\n",
      "  eval: bucket 3 perplexity 33.61\n",
      "global step 6600 learning rate 0.4950 step-time 5.80 perplexity 25.70\n",
      "  eval: bucket 0 perplexity 2.08\n",
      "  eval: bucket 1 perplexity 10.25\n",
      "  eval: bucket 2 perplexity 22.91\n",
      "  eval: bucket 3 perplexity 35.85\n",
      "global step 6800 learning rate 0.4950 step-time 5.89 perplexity 24.95\n",
      "  eval: bucket 0 perplexity 2.71\n",
      "  eval: bucket 1 perplexity 9.97\n",
      "  eval: bucket 2 perplexity 20.29\n",
      "  eval: bucket 3 perplexity 33.37\n",
      "global step 7000 learning rate 0.4950 step-time 5.94 perplexity 23.43\n",
      "  eval: bucket 0 perplexity 2.70\n",
      "  eval: bucket 1 perplexity 8.03\n",
      "  eval: bucket 2 perplexity 20.18\n",
      "  eval: bucket 3 perplexity 28.00\n",
      "global step 7200 learning rate 0.4950 step-time 5.36 perplexity 20.94\n",
      "  eval: bucket 0 perplexity 2.99\n",
      "  eval: bucket 1 perplexity 8.09\n",
      "  eval: bucket 2 perplexity 19.84\n",
      "  eval: bucket 3 perplexity 29.94\n",
      "global step 7400 learning rate 0.4950 step-time 6.19 perplexity 22.80\n",
      "  eval: bucket 0 perplexity 2.79\n",
      "  eval: bucket 1 perplexity 9.74\n",
      "  eval: bucket 2 perplexity 17.66\n",
      "  eval: bucket 3 perplexity 30.39\n",
      "global step 7600 learning rate 0.4950 step-time 5.63 perplexity 21.19\n",
      "  eval: bucket 0 perplexity 3.18\n",
      "  eval: bucket 1 perplexity 9.38\n",
      "  eval: bucket 2 perplexity 16.67\n",
      "  eval: bucket 3 perplexity 26.35\n",
      "global step 7800 learning rate 0.4950 step-time 5.90 perplexity 19.73\n",
      "  eval: bucket 0 perplexity 2.53\n",
      "  eval: bucket 1 perplexity 7.14\n",
      "  eval: bucket 2 perplexity 16.33\n",
      "  eval: bucket 3 perplexity 29.06\n",
      "global step 8000 learning rate 0.4950 step-time 5.66 perplexity 19.06\n",
      "  eval: bucket 0 perplexity 3.18\n",
      "  eval: bucket 1 perplexity 8.75\n",
      "  eval: bucket 2 perplexity 16.80\n",
      "  eval: bucket 3 perplexity 25.20\n",
      "global step 8200 learning rate 0.4950 step-time 5.40 perplexity 19.01\n",
      "  eval: bucket 0 perplexity 2.95\n",
      "  eval: bucket 1 perplexity 9.21\n",
      "  eval: bucket 2 perplexity 14.23\n",
      "  eval: bucket 3 perplexity 25.82\n",
      "global step 8400 learning rate 0.4950 step-time 5.47 perplexity 18.41\n",
      "  eval: bucket 0 perplexity 2.55\n",
      "  eval: bucket 1 perplexity 6.42\n",
      "  eval: bucket 2 perplexity 16.32\n",
      "  eval: bucket 3 perplexity 24.46\n",
      "global step 8600 learning rate 0.4950 step-time 5.53 perplexity 17.45\n",
      "  eval: bucket 0 perplexity 2.13\n",
      "  eval: bucket 1 perplexity 8.03\n",
      "  eval: bucket 2 perplexity 13.08\n",
      "  eval: bucket 3 perplexity 20.83\n",
      "global step 8800 learning rate 0.4950 step-time 5.70 perplexity 17.21\n",
      "  eval: bucket 0 perplexity 2.26\n",
      "  eval: bucket 1 perplexity 6.46\n",
      "  eval: bucket 2 perplexity 16.53\n",
      "  eval: bucket 3 perplexity 22.98\n",
      "global step 9000 learning rate 0.4950 step-time 5.55 perplexity 16.55\n",
      "  eval: bucket 0 perplexity 2.32\n",
      "  eval: bucket 1 perplexity 6.84\n",
      "  eval: bucket 2 perplexity 11.76\n",
      "  eval: bucket 3 perplexity 23.57\n",
      "global step 9200 learning rate 0.4950 step-time 5.61 perplexity 16.42\n",
      "  eval: bucket 0 perplexity 1.97\n",
      "  eval: bucket 1 perplexity 6.29\n",
      "  eval: bucket 2 perplexity 13.20\n",
      "  eval: bucket 3 perplexity 19.67\n",
      "global step 9400 learning rate 0.4950 step-time 5.46 perplexity 15.34\n",
      "  eval: bucket 0 perplexity 2.51\n",
      "  eval: bucket 1 perplexity 5.47\n",
      "  eval: bucket 2 perplexity 12.27\n",
      "  eval: bucket 3 perplexity 22.21\n",
      "global step 9600 learning rate 0.4950 step-time 6.18 perplexity 16.35\n",
      "  eval: bucket 0 perplexity 2.06\n",
      "  eval: bucket 1 perplexity 5.09\n",
      "  eval: bucket 2 perplexity 14.25\n",
      "  eval: bucket 3 perplexity 19.41\n",
      "global step 9800 learning rate 0.4950 step-time 5.46 perplexity 14.87\n",
      "  eval: bucket 0 perplexity 1.87\n",
      "  eval: bucket 1 perplexity 6.30\n",
      "  eval: bucket 2 perplexity 12.07\n",
      "  eval: bucket 3 perplexity 21.33\n",
      "global step 10000 learning rate 0.4950 step-time 5.46 perplexity 14.70\n",
      "  eval: bucket 0 perplexity 2.07\n",
      "  eval: bucket 1 perplexity 6.07\n",
      "  eval: bucket 2 perplexity 11.96\n",
      "  eval: bucket 3 perplexity 20.20\n",
      "global step 10200 learning rate 0.4950 step-time 5.62 perplexity 14.33\n",
      "  eval: bucket 0 perplexity 2.04\n",
      "  eval: bucket 1 perplexity 5.94\n",
      "  eval: bucket 2 perplexity 14.56\n",
      "  eval: bucket 3 perplexity 17.55\n",
      "global step 10400 learning rate 0.4950 step-time 5.62 perplexity 14.23\n",
      "  eval: bucket 0 perplexity 2.22\n",
      "  eval: bucket 1 perplexity 6.62\n",
      "  eval: bucket 2 perplexity 10.96\n",
      "  eval: bucket 3 perplexity 19.54\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1fe697abea81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-85b079676242>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m           data, bucket_id)\n\u001b[1;32m     63\u001b[0m       _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[0;32m---> 64\u001b[0;31m                                    target_weights, bucket_id, False)\n\u001b[0m\u001b[1;32m     65\u001b[0m       \u001b[0mstep_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(sentence):\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    enWords = []\n",
    "    with open(\"vocab.en\") as enVocabFile:\n",
    "      enWords.extend(enVocabFile.readlines())\n",
    "    enWords = [line.strip() for line in enWords]\n",
    "    enVocab = dict([(x, y) for (y, x) in enumerate(enWords)])\n",
    "    \n",
    "    ptWords = []\n",
    "    with open(\"vocab.pt\") as ptVocabFile:\n",
    "      ptWords.extend(ptVocabFile.readlines())\n",
    "    ptWords = [line.strip() for line in ptWords]\n",
    "    ptVocab = dict(enumerate(ptWords))\n",
    "\n",
    "    # Get token-ids for the input sentence.\n",
    "    tokens = word_tokenize(sentence, language=\"English\")\n",
    "    tokenIds = [enVocab.get(t, UNK_ID) for t in tokens]\n",
    "\n",
    "    # Which bucket does it belong to?\n",
    "    bucket_id = len(_buckets) - 1\n",
    "    for i, bucket in enumerate(_buckets):\n",
    "      if bucket[0] >= len(tokenIds):\n",
    "        bucket_id = i\n",
    "        break\n",
    "    else:\n",
    "      logging.warning(\"Sentence truncated: %s\", sentence)\n",
    "\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "        {bucket_id: [(tokenIds, [])]}, bucket_id)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                     target_weights, bucket_id, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if EOS_ID in outputs:\n",
    "      outputs = outputs[:outputs.index(EOS_ID)]\n",
    "    # Print out Portuguese sentence corresponding to outputs.\n",
    "    print(\" \".join([tf.compat.as_str(ptVocab[output]) for output in outputs]))\n",
    "    print(\"> \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from checkpoint/translate.ckpt-86200\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Restore called with invalid save path: 'checkpoint/translate.ckpt-86200'. File path is: 'checkpoint/translate.ckpt-86200'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-94d9428fab47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"My day is good, how about you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d0827046f38a>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Create model and load parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# We decode one sentence at a time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-233ca253552e>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(session, forward_only)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading model parameters from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created model with fresh parameters.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/final/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m         raise ValueError(\"Restore called with invalid save path: %r. \"\n\u001b[0;32m-> 1342\u001b[0;31m                          \"File path is: %r\" % (save_path, file_path))\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     sess.run(self.saver_def.restore_op_name,\n",
      "\u001b[0;31mValueError\u001b[0m: Restore called with invalid save path: 'checkpoint/translate.ckpt-86200'. File path is: 'checkpoint/translate.ckpt-86200'"
     ]
    }
   ],
   "source": [
    "decode(\"My day is good, how about you?\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:final]",
   "language": "python",
   "name": "conda-env-final-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
